---
title: "Análisis de datos"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

Se presenta todo lo que se hizo para llegar a los resultados. Muchas cosas son más de estadística que del proyecto, sin embargo, es mejor que no falte. 

# Preparación

## Paquetes

```{r}
library(readxl)
library(lme4)
library(dplyr)
library(ggplot2)
library(qqplotr)
library(car)
library(lattice)
library(emmeans)
```

Tengo entendido que para la tesis es necesario colocar los paquetes en algún lado, puede hacer la consulta. 


Se cargan los datos. Es necesario pasar las observaciones a tipo factor.

```{r}
base1=read_xlsx("final_registro_cloud.xlsx",col_names = FALSE)
names(base1)=c("Tiempo","BL","Nube","Denied","Right","Solicitudes","ID","Bloque")
base1$BL=as.factor(base1$BL)
base1$Solicitudes=as.factor(base1$Solicitudes)
base1$Nube=as.factor(base1$Nube)




base2=read_xlsx("final_registro_local.xlsx",col_names = FALSE)
names(base2)=c("Tiempo","BL","Nube","Denied","Right","Solicitudes","ID","Bloque")
base2$BL=as.factor(base2$BL)
base2$Solicitudes=as.factor(base2$Solicitudes)
base2$Nube=as.factor(base2$Nube)
```

Podemos unir las bases para que sea solo una tabla grande y ver los tratamientos

```{r}
base=rbind(base1,base2)
base$Bloque=as.factor(base$Bloque)
table(base$BL,base$Nube,base$Solicitudes)
str(base)
```

Vemos que hay un desbalance con los tratamientos, según se pueden eliminar parcelas, se eliminan las que causan el desbalance. 

```{r}
quitar1=which(base1$Bloque=="N109")
base1=base1[-quitar1,]

quitar2=which(base1$Bloque=="N1009")
base1=base1[-quitar2,]

quitar3=which(base1$Bloque=="N5009")
base1=base1[-quitar3,]

quitar4=which(base1$Bloque=="N5006")
base1=base1[-quitar4,]

quitar5=which(base2$Bloque=="L5006")
base2=base2[-quitar5,]

quitar6=which(base1$Bloque=="N5008")
base1=base1[-quitar6,]

quitar7=which(base2$Bloque=="L5008")
base2=base2[-quitar7,]

quitar8=which(base1$Bloque=="N5004")
base1=base1[-quitar8,]

quitar9=which(base2$Bloque=="L5004")
base2=base2[-quitar9,]

quitar10=which(base1$Bloque=="N1007")
base1=base1[-quitar10,]

quitar11=which(base2$Bloque=="L1007")
base2=base2[-quitar11,]

base2$Bloque=base1$Bloque
base=rbind(base1,base2)
table(base$BL,base$Nube,base$Solicitudes)
```

Se soluciona el desbalance, ahora los bloques deben de coincidir.

# Estadística Descriptiva.

## Promedios por tratamiento y varianza. 

```{r}
options(scipen = 999)
m=tapply(base$Tiempo,list(base$BL,base$Nube,base$Solicitudes),mean);m
v=tapply(base$Tiempo,list(base$BL,base$Nube,base$Solicitudes),var);v
```

Observamos que son valores que están cercanos a 0, esto se debe a que la variable respuesta está dada en segundos y pero los valores de tiempo son mucho más pequeños que un segundo.

```{r}
base$TiempoT=base$Tiempo*100
m=tapply(base$TiempoT,list(base$BL,base$Nube,base$Solicitudes),mean);m
v=tapply(base$TiempoT,list(base$BL,base$Nube,base$Solicitudes),var);v
```

Parece que en *centisegundos* todo tiene un poco más de coherencia.

## Análisis gráfico 


### Gráfico de cajas 

Este puede ayudarnos a comprender los tratamientos, como se comparan entre ellos en términos de medias y varianzas. Como se trata de un modelo con bloques hay que centras los datos. 

```{r}
modc=lm(TiempoT~Bloque,data = base) 
fit=modc$fit
rc=base$TiempoT-fit+mean(base$TiempoT)
base$rc=rc


ggplot(base, aes(x = interaction(BL, Nube, Solicitudes), y = rc)) +
  geom_point() +
  labs(x = "Tratamiento", y = "Tiempo Centrado",title = "Observaciones y promedio por tratamiento") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  stat_summary(fun.y = mean, geom = "point", color = "red", size = 1.5)+
  geom_hline(yintercept = mean(rc), linetype = "dashed", color = "blue")
```

Observamos un gráfico de cajas, la lineas negras representan el promedio de cada tratamiento (cada combinación de cada nivel de cada factor)

### Gráfico de lineas para ver Interacción Triple. 

```{r}
xyplot(base$TiempoT~base$BL|base$Solicitudes,group=base$Nube,pch=18,type="a",xlab="Método de detección",ylab="Tiempo en centisegundos",main="Gráfico de interacción triple")
```

Gráficamente no se observa interacciónm triple, siemore es necesaria una prueba estadística para confirmarlo pero es un buen indicio. 


### Gráficos de lineas para ver interacción doble. 

```{r}
ggplot(base,aes(x=Nube,y=TiempoT,group = BL))+
  stat_summary(fun.y = "mean",geom = "line",aes(linetype = BL))+labs(title="Interacción entre Ubicación y Método")

ggplot(base,aes(x=Solicitudes,y=TiempoT,group = BL))+
  stat_summary(fun.y = "mean",geom = "line",aes(linetype = BL))+labs(title="Interacción entre Solicitudes y Método")

ggplot(base,aes(x=Solicitudes,y=TiempoT,group = Nube))+
  stat_summary(fun.y = "mean",geom = "line",aes(linetype = Nube))+labs(title="Interacción entre Ubicación y Solicitudes")
  
```

Solo se observa interacción entre Ubicación y Método de detección, vemos que las diferencias entre Black List y Machine Learning son más pequeñas en la Nube que en Local. 

# El modelo

El modelo es un modelo mixto de parcelas divididas, el cual debe de cumplir los supuestos de normalidad y de homocedasticidad. Este no se trata del modelo completo (Adjunto más adelante)

Es el siguiente:

$$
\mu_{ijk}=\mu+\alpha_i+\beta_j+\delta_k+(\alpha \beta)_{ij}+(\alpha\delta)_{ik}+(\beta \delta)_{jk}+(\alpha \beta \delta)_{ijk}+\nu_l
$$ 


Donde:

-   $\mu$ es la media general
-   $\alpha_i$ es el efecto del nivel i del Factor BlackList
-   $\beta_j$ es el efecto del nivel j del Factor Nube
-   $\delta_k$ es el efecto del nivel k del Factor Solicitudes
-   $(\alpha \beta)_{ij}$ es el efecto de la interacción entre BlackList y Nube
-   $(\alpha \delta)_{ik}$ es el efecto de la interacción entre BlackList y Solicitudes
-   $(\beta \delta)_{jk}$ es el efecto de la interacción entre Nube y Solicitudes
-   $\nu_l$ Es el efecto de la parcela. 

Se ajusta un modelo mixto

```{r}
modp0=lmer(TiempoT~BL*Nube*Solicitudes+(1|Bloque),data = base)
summ0=summary(modp0)
```

## Supuestos del modelo 

**A partir de acá es pura estadística.** Lo importante es comprender lo que hablamos, los modelos asumen ciertos elementos sobre los datos y estos datos, de la forma en los que los teníamos no cumplían ningún supuesto. 

Se asume el supuesto de independencia, se analiza entonces homocedasticidad y normalidad. Se oueden usar qqPlots, la prueba de Kolmogorov-Smirnov, el gráfico de predichos contra residuos y la prueba de levene.

```{r}
res0=summ0$residuals
fit0=fitted(modp0)
dffp=data.frame(res0,fit0)


ggplot(data = dffp, aes(x = fit0, y = res0)) +
  geom_point()


leveneTest(res0~BL*Nube*Solicitudes,data = base)
```

Tanto en el gráfico de residuos contra predichos como en la prueba de Levene, no hay razón para creer que hay homocedasticidad. 

```{r}
ks.test(res0,"pnorm")

residuos=as.data.frame(res0)
res.stnd = scale(residuos$res0) # <- Residuos estandarizados. 
ggplot(data = residuos, mapping = aes(sample = res.stnd )) + stat_qq_point() +                          stat_qq_line(color="blue") + stat_qq_band(fill=NA, color="blue") +
      ylab("Residuos Estandarizados") + xlab("Norm Quantiles") + ggtitle("Normal QQ Plot")  +             labs(caption = "Grafico 1")+ theme_classic()+ ylim(-100, 100)
```

De igual forma, tanto en el gráfico como en la prueba de Kolmogorov se puede distinguir normalidad.

En ambos gráficos hay un dato extremo.

Vamos a intentar cambiar la respuesta por el promedio del tratamiento al que pertenecen 

```{r}
which(res0>20,)
#tapply(base$TiempoT,list(base$BL,base$Nube,base$Solicitudes),mean) ##El promedio es 19 y estos tienen un valor mayor a 100
baseA=base
baseA[c(5325,5327),9]=c(19.18961,19.18961)



```

Podemos volver a hacer todo a ver qué tal nos va

```{r}
modp0=lmer(TiempoT~BL*Nube*Solicitudes+(1|Bloque),data = baseA)
summ0=summary(modp0)

# Homocedasticidad  

res0=summ0$residuals
fit0=fitted(modp0)
dffp=data.frame(res0,fit0)


ggplot(data = dffp, aes(x = fit0, y = res0)) +
  geom_point()


leveneTest(res0~BL*Nube*Solicitudes,data = baseA)

# Normalidad 

ks.test(res0,"pnorm")

residuos=as.data.frame(res0)
res.stnd = scale(residuos$res0) # <- Residuos estandarizados. 
ggplot(data = residuos, mapping = aes(sample = res.stnd )) + stat_qq_point() +                          stat_qq_line(color="blue") + stat_qq_band(fill=NA, color="blue") +
      ylab("Residuos Estandarizados") + xlab("Norm Quantiles") + ggtitle("Normal QQ Plot")  +             labs(caption = "Grafico 1")+ theme_classic()+ ylim(-100, 100)


```
No mejoró mucho 

Por esta razón vamos a intentar a justar un modelo Generalizado mixto, a ver si eso nos ayuda a arreglar los supuestos.


# Modelo generalizado Mixto 

De igual forma se probó con un modelo más complejo y no se obtuvo ningún resultado distinto. 

```{r}
modp1=glmer(TiempoT~BL*Nube*Solicitudes+(1|Bloque),family = Gamma(link = "log"),data = base)
summ=summary(modp1)
summ
```

## Supuestos

```{r}
res1=summ$residuals
fit1=fitted(modp1)
dffp1=data.frame(res1,fit1)


ggplot(data = dffp, aes(x = fit1, y = res1)) +
  geom_point()


leveneTest(res1~BL*Nube*Solicitudes,data = base)
```


```{r}
ks.test(res1,"pnorm")

residuos1=as.data.frame(res1)
res.stnd1 = scale(residuos1$res1) # <- Residuos estandarizados. 
ggplot(data = residuos1, mapping = aes(sample = res.stnd1 )) + stat_qq_point() +                          stat_qq_line(color="blue") + stat_qq_band(fill=NA, color="blue") +
      ylab("Residuos Estandarizados") + xlab("Norm Quantiles") + ggtitle("Normal QQ Plot")  +             labs(caption = "Grafico 1")+ theme_classic()+ ylim(-100, 100)
```

Observamos que la normalidad se arregla un poco, pero la homocedasticidad sigue dando problemas. Puede que los valores extemos sigan afectando. 

En este caso hay un punto en especifico que causa problemas, se va a cambiar por el promedio del tratamiento 

```{r}
which(res1>75);which(res.stnd1>75) ## Se trata del mismo 

base[1465,]
0.09379336  ## Es el promedio del tratamiento que es BL,Nube y 500 y Tarda 9!

baseB=base
baseB[1465,9]=0.09379336


```

Y podemos volver a hacer todo a ver como nos da. 


```{r}
modp1=glmer(TiempoT~BL*Nube*Solicitudes+(1|Bloque),family = Gamma(link = "log"),data = baseB)

# Homocedasticidad 

res1=summ$residuals
fit1=fitted(modp1)
dffp1=data.frame(res1,fit1)


ggplot(data = dffp, aes(x = fit1, y = res1)) +
  geom_point()


leveneTest(res1~BL*Nube*Solicitudes,data = base)

# Normalidad 

ks.test(res1,"pnorm")

residuos1=as.data.frame(res1)
res.stnd1 = scale(residuos1$res1) # <- Residuos estandarizados. 
ggplot(data = residuos1, mapping = aes(sample = res.stnd1 )) + stat_qq_point() +                          stat_qq_line(color="blue") + stat_qq_band(fill=NA, color="blue") +
      ylab("Residuos Estandarizados") + xlab("Norm Quantiles") + ggtitle("Normal QQ Plot")  + labs(caption = "Grafico 1")+ theme_classic()+ ylim(-100, 100)

summary(modp1)

```

Ningún modelo con los datos como estaban resultó satisfactorio y puede llevarnos a cometer errores.

Se decide explorar alternativas un enfoque diferente. 


Se debe promediar la información de las URL de cada subparcela(lo que significa es que dentro de cada bloque de 10,100,500 URL se toma el promedio por tratamiento, por lo que cada bloque ahora tienen 4 respuestas, más abajo imprimo la base para que la vea a detalle). Se obtiene tanto el promedio como la mediana. Voy a modelar el promedio, pero hagan lo mismo con la mediana a ver.

```{r}
rm(base1);rm(base2)
save(base,file = "base.Rdata")
base2 <- base %>%
  group_by(Bloque, BL,Nube, Solicitudes) %>%
  summarise(promedio=mean(TiempoT),mediana=median(TiempoT))

base2$Bloque=as.factor(base2$Bloque)
base2
```



```{r}
basemod=base2
modp0=lmer(promedio~BL*Nube*Solicitudes+(1|Bloque),data = base2)
drop1(modp0,test = "Chisq")
modp1=lmer(promedio~BL*Nube+Nube*Solicitudes+BL*Solicitudes+(1|Bloque),data = base2)
drop1(modp1,test = "Chisq")
modp1=lmer(promedio~BL*Nube+Nube*Solicitudes+(1|Bloque),data = base2)
drop1(modp1,test = "Chisq")
modp1=lmer(promedio~BL*Nube+Solicitudes+(1|Bloque),data = base2)
drop1(modp1,test = "Chisq")
summ1=summary(modp1)

res1=summ1$residuals
fit1=fitted(modp1)
dffp=data.frame(res1,fit1)
plot(fit1,res1)

leveneTest(promedio~BL*Nube*Solicitudes,data = base2)
leveneTest(res1~BL*Nube*Solicitudes,data = base2)

ks.test(res1,"pnorm")

residuos=as.data.frame(res1)
res.stnd = scale(residuos$res1) # <- Residuos estandarizados. 
qqPlot(res1)
qqPlot(res.stnd)

```

Aquí pasan varias cosas, una es que hay dos parcelas con valores muy distintos al resto. Se puede quitar esas parcelas o asignarle la mediana. Sería al tratamiento ML Local de N1002, y N107 ML Nube. La otra es que aunque con Levene no se rechaza la hipótesis, se ve en el gráfico de ajustados y residuos que no hay homoscedasticidad. 
La opción es quitar esas dos parcelas y agregar pesos.

De nuevo, esto es pura estadística.

```{r}
base2=base2[-which(base2$Bloque=="N107"|base2$Bloque=="N1002"),]
v=tapply(base2$promedio,list(base2$BL,base2$Nube,base2$Solicitudes),var);v

pesos=data.frame(Solicitudes=rep(c(10,100,500),each=4),
                 Nube=rep(rep(c("Nube","Local"),each=2),3),
                 BL=rep(c("BL","ML"),6),w=as.vector(v))
base3=merge(base2,pesos,by=c("BL","Nube","Solicitudes"))
basemod=base3
modp1=lmer(promedio~BL*Nube+Solicitudes+(1|Bloque),weights = 1/w,data = basemod)
basemod
drop1(modp1,test = "Chisq")
summary(modp1)
confint(modp1)
summ1=summary(modp1)

res1=summ1$residuals
fit1=fitted(modp1)
dffp=data.frame(res1,fit1)
plot(fit1,res1)

#Esto ya no se revisa porque se trabaja considerando heteroscedasticidad
# leveneTest(promedio~BL*Nube*Solicitudes,data = basemod)
# leveneTest(res1~BL*Nube*Solicitudes,data = basemod)

ks.test(res1,"pnorm")

residuos=as.data.frame(res1)
res.stnd = scale(residuos$res1) # <- Residuos estandarizados. 
qqPlot(res1)
qqPlot(res.stnd)


contlmer=emmeans (modp1,  pairwise~ BL | Nube, adjust="bonferroni");contlmer



```

Este de arriba sería el modelo final, hay normalidad, no hay homoscedasticidad pero se está considerando la heteroscedasticidad al incluir pesos y se hace una mejor estimación del error estándar para las comparaciones. Hay que calcular cotas inferiores y concluir con respecto a una diferencia relevante, porque ambas son significativas. 


Este sería todo el procedimiento que se usó para llegar al modelo. El cual es el siguiente.

# El modelo final

$$
\mu_{ijk}=\mu+\alpha_i+\beta_j+\delta_k+(\alpha \beta)_{ij}+\nu_l
$$ 


Donde:

-   $\mu$ es la media general, es decir, sin tomar en cuenta ningún tratamiento. 
-   $\alpha_i$ es el efecto del nivel i del Factor BlackList (Método de detección)
-   $\beta_j$ es el efecto del nivel j del Factor Nube (Ubicación)
-   $\delta_k$ es el efecto del nivel k del Factor Solicitudes
-   $(\alpha \beta)_{ij}$ es el efecto de la interacción entre BlackList y Nube
-   $\nu_l$ Es el efecto de la parcela. Es decir, el efecto que cada bloque de 10,100,500 trae sobre el modelo. 



Ahora se hacen las pruebas de hipótesis 

```{r}
contlmer=emmeans (modp1,  pairwise~ BL | Nube, adjust="bonferroni");contlmer
```


Y se calculan los intervalos. 

```{r}
confint(contlmer)
```


Más sobre los resultados en el Word. 














